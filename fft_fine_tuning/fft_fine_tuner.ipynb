{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText FNTR\n",
    "\n",
    "On this notebook we will see how to retrain a FastText model already pretrained using the **GENSIM API**.\n",
    "To accomplis this task we will:\n",
    "\n",
    "<li> Load a file with spanish text from my own web scrapping script.\n",
    "<li> Preprocess just a bit this text in order to fed into the model.\n",
    "<li> Fine tune a pretrained model using callbacks.\n",
    "<li> Check if the model has learnt anything.\n",
    "    \n",
    "Some tests have been already performed in order to find the proper way to do this. On previous expermients we have observed that this is very influenced by stopwords, that **fasttext pretrained model contains stopwords and accent marks and it is previously lowercased**. We will clean both in order not to introduce much noise to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and  Load Text to fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaae', 'eeeiii', 'oooo', 'amigo', 'entr']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def c_ac(word: str) -> str:\n",
    "    return word.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace(',', '')\n",
    "\n",
    "def clean_acmarks_and_sws(frase: str) ->  list:\n",
    "    frase = frase.split(' ')\n",
    "    sws = stopwords.words('spanish')\n",
    "    return [c_ac(word) for word in frase if word not in sws]\n",
    "\n",
    "\n",
    "# clean_acmarks('áérewrewiíiiiíi', sws)\n",
    "clean_acmarks_and_sws('aaááé eeeiíí ooóó a ante el de la amigo, entr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 [['llega', 'verano', 'vacaciones', 'el', 'dividendos'], ['junio', 'julio', 'muchas', 'cotizadas', 'españolas', 'optan', 'premiar', 'fidelidad', 'accionistas', 'mediante', 'pago', 'cupon'], ['tras', 'verano', 'atipico', 'vivio', '2020', 'culpa', 'crisis', 'covid-19', 'llevo', 'muchas', 'compañias', 'suspender', 'pagos', 'recortarlos', 'muchas', 'variar', 'calendarios', 'repartos', 'verano', 'cotizadas', 'haran', 'esfuerzo', 'ir', 'recuperando', 'normalidad', 'politicas', 'retribucion', 'accionista', 'medida', 'ido', 'reactivando', 'ingresos', 'beneficios.en', 'proximas', 'semanas', 'veintena', 'compañias', 'españolas', 'repartiran', '5.000', 'millones', 'euros', 'dividendos'], ['cifra', 'añadir', 'desembolsados', 'mes', 'junio', 'superan', '2.000', 'millones', 'gracias', 'parte', '1.000', 'millones', 'repartidos', 'telefonica', 'ultimo', 'dividendo', 'desembolsos', 'realizados', 'caixabank', 'hizo', 'primer', 'pago', 'tras', 'integracion', 'bankia', 'grifols', 'acerinox.en', 'proximas', 'semanas', 'valores', 'importantes', 'ibex', 'cita', 'accionistas'], ['dejando', 'lado', 'cupon', 'telefonica', 'junio', 'proximas', 'semanas', 'mayor', 'desembolso', 'correra', 'cargo', 'iberdrola']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file = os.path.join(os.getcwd(), 'FastText_FINE', 'mercados_out.txt')\n",
    "\n",
    "with open(file, 'r', encoding = 'latin-1') as file:\n",
    "    txt = file.read()\n",
    "    \n",
    "txt_split_tokenized = [clean_acmarks_and_sws(frase) for frase in txt.split('. ')]\n",
    "txt_split_tokenized = txt_split_tokenized[:18000]\n",
    "print(len(txt_split_tokenized), txt_split_tokenized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    def on_epoch_begin(self, model):\n",
    "        if self.epoch % 100 == 0:\n",
    "            print(\"Epoch #{} start\".format(self.epoch))\n",
    "    def on_epoch_end(self, model):\n",
    "        if self.epoch % 100 == 0:\n",
    "            print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial check of how does FastText thinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.wrappers import FastText\n",
    "from gensim.models.fasttext import FastText\n",
    "import gensim\n",
    "\n",
    "fasttext = gensim.models.fasttext.load_facebook_model(r'C:\\Users\\f.gonzalez\\Desktop\\FastText_FINE\\wiki.es.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study the initial results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mercado\n",
      "[('mercado,', 0.7775014042854309), ('mercados', 0.7754110097885132), ('posmercado', 0.7551040649414062), ('#mercado', 0.7441679239273071), ('‘mercado', 0.7340843677520752), ('mercada', 0.7078138589859009), ('promercado', 0.7052767276763916)]\n",
      " \n",
      "bolsa\n",
      "[('bolsas', 0.7388685941696167), ('bolsada', 0.6469042301177979), ('bolsa»', 0.6271626353263855), ('cotiza', 0.6204637289047241), ('bursátil', 0.6049832701683044), ('embolsa', 0.6042190790176392), ('nasdaq', 0.5938200354576111)]\n",
      " \n",
      "Bolsa\n",
      "[('olsa', 0.818496823310852), ('colsa', 0.7439507842063904), ('tolsa', 0.7165077924728394), ('dolsa', 0.7157090306282043), ('polsa', 0.7060896158218384), ('bupolsa', 0.6889318227767944), ('molsa', 0.6666685342788696)]\n",
      " \n",
      "banco\n",
      "[('ibanco', 0.7881178259849548), ('bancos', 0.7591121792793274), ('bancoop', 0.7470996379852295), ('mibanco', 0.7466395497322083), ('bancor', 0.7200927138328552), ('bancaria', 0.716255784034729), ('bancario', 0.7133525609970093)]\n",
      " \n",
      "Banco\n",
      "[('yanco', 0.6898605227470398), ('ñanco', 0.6889960169792175), ('pichanco', 0.6823719143867493), ('sanco', 0.6783881783485413), ('canco', 0.6665889024734497), ('chanco', 0.6644322872161865), ('panco', 0.6639772653579712)]\n",
      " \n",
      "cárcel\n",
      "[('prisión', 0.8898036479949951), ('carcela', 0.7493693828582764), ('preso', 0.7421814203262329), ('cárcel—', 0.7421325445175171), ('encarcelado', 0.7370400428771973), ('cárcel»', 0.7323863506317139), ('encarcelada', 0.7309428453445435)]\n",
      " \n",
      "empresa\n",
      "[('—empresa', 0.7761496901512146), ('empresa–', 0.7629649043083191), ('empresa,', 0.7604799270629883), ('empresas,', 0.7268840074539185), ('compañía', 0.7260661721229553), ('subsidiaria', 0.7203832864761353), ('empres', 0.6929910182952881)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for word in ['mercado', 'bolsa', 'Bolsa', 'banco', 'Banco', 'cárcel', 'empresa']:\n",
    "    print(word)\n",
    "    print(fasttext.wv.most_similar(word)[:7])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), (300,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mercado, bolsa = fasttext.wv['mercado'], fasttext.wv['bolsa']\n",
    "mercado.shape, bolsa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this model has been clearly trained with lowercased text and with punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #100 start\n",
      "Epoch #100 end\n",
      "Epoch #200 start\n",
      "Epoch #200 end\n",
      "Epoch #300 start\n",
      "Epoch #300 end\n",
      "Epoch #400 start\n",
      "Epoch #400 end\n",
      "Epoch #500 start\n",
      "Epoch #500 end\n",
      "Epoch #600 start\n",
      "Epoch #600 end\n",
      "Epoch #700 start\n",
      "Epoch #700 end\n",
      "Epoch #800 start\n",
      "Epoch #800 end\n",
      "Epoch #900 start\n",
      "Epoch #900 end\n",
      "Epoch #1000 start\n",
      "Epoch #1000 end\n",
      "Epoch #1100 start\n",
      "Epoch #1100 end\n",
      "Epoch #1200 start\n",
      "Epoch #1200 end\n",
      "Epoch #1300 start\n",
      "Epoch #1300 end\n",
      "Epoch #1400 start\n",
      "Epoch #1400 end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2:17:25'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epoch_logger = EpochLogger()\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "new_sentences = txt_split_tokenized\n",
    "\n",
    "fasttext.build_vocab(new_sentences, update=True)\n",
    "fasttext.train(new_sentences, total_examples=len(new_sentences), epochs = 1500, callbacks=[epoch_logger])\n",
    "\n",
    "elapsed = (time.time() - start)\n",
    "\n",
    "str(timedelta(seconds=elapsed))[:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has the model learnt anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.allclose(fasttext.wv['mercado'], mercado), np.allclose(fasttext.wv['bolsa'], bolsa) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mercado\n",
    "[('mercado,', 0.7775014042854309), ('mercados', 0.7754110097885132), ('posmercado', 0.7551040649414062), ('#mercado', 0.7441679239273071), ('‘mercado', 0.7340843677520752), ('mercada', 0.7078138589859009), ('promercado', 0.7052767276763916)]\n",
    " \n",
    "bolsa\n",
    "[('bolsas', 0.7388685941696167), ('bolsada', 0.6469042301177979), ('bolsa»', 0.6271626353263855), ('cotiza', 0.6204637289047241), ('bursátil', 0.6049832701683044), ('embolsa', 0.6042190790176392), ('nasdaq', 0.5938200354576111)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mercado\n",
      "[('año', 0.6514887809753418), ('deuda', 0.6298579573631287), ('inversores', 0.627924919128418), ('bolsa', 0.6208838224411011), ('mercados', 0.6208115816116333), ('parte', 0.5797388553619385), ('valores', 0.5716472864151001)]\n",
      " \n",
      "bolsa\n",
      "[('española', 0.7075108289718628), ('año', 0.621670663356781), ('mercado', 0.6208838224411011), ('inversores', 0.5761557817459106), ('compañia', 0.5678509473800659), ('deuda', 0.5638283491134644), ('salida', 0.5417485237121582)]\n",
      " \n",
      "Bolsa\n",
      "[('olsa', 0.8153858780860901), ('colsa', 0.6973823308944702), ('nlsa', 0.6802817583084106), ('dolsa', 0.6668431758880615), ('tolsa', 0.6642357110977173), ('polsa', 0.6628522872924805), ('molsa', 0.6579697132110596)]\n",
      " \n",
      "banco\n",
      "[('central', 0.6823917031288147), ('ibanco', 0.5623183846473694), ('españa', 0.5554822087287903), ('/banco', 0.539932131767273), ('europeo', 0.5375440120697021), ('bancoop', 0.5332126617431641), ('bancoldex', 0.5326253175735474)]\n",
      " \n",
      "Banco\n",
      "[('yanco', 0.833631157875061), ('anco', 0.8321518898010254), ('ñanco', 0.8188300132751465), ('hanco', 0.7905027866363525), ('lyanco', 0.7794116139411926), ('panco', 0.7687035799026489), ('lanco', 0.7634850740432739)]\n",
      " \n",
      "cárcel\n",
      "[('cárcel—', 0.7990392446517944), ('carcel', 0.7828296422958374), ('cárcel»', 0.7706230878829956), ('lacárcel', 0.7633313536643982), ('carcelería', 0.7591214776039124), ('carceles', 0.7579731941223145), ('carcela', 0.7565960884094238)]\n",
      " \n",
      "empresa\n",
      "[('—empresa', 0.6465967893600464), ('empresa,', 0.6304169297218323), ('empresa–', 0.5940812826156616), ('empresa—', 0.5701382160186768), ('subempresa', 0.5654283761978149), ('compañia', 0.5606735348701477), ('#empresa', 0.5516312122344971)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for word in ['mercado', 'bolsa', 'Bolsa', 'banco', 'Banco', 'cárcel', 'empresa']:\n",
    "    print(word)\n",
    "    print(fasttext.wv.most_similar(word)[:7])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist Model to disk and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = os.path.join(os.getcwd(), 'FastText_FINE', 'fastext_mercados_fine_tuned-')\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix=name, delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    fasttext.save(temporary_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
