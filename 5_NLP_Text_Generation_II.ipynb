{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi | grep \"Tesla\\|MiB\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmTpIYS-2opf",
        "outputId": "0550d2af-8036-4d95-add3-3a95e596ce20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "weights = subprocess.getoutput('ls | grep \"weights\"')\n",
        "print(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPuoxjyqDqtr",
        "outputId": "d9c4d81b-fc88-456e-820d-a3590c35bea5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights-improvement-09-1.1340-bigger.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First input file\n",
        "\n",
        "Las inquietudes de Shanti Andia."
      ],
      "metadata": {
        "id": "zVs1fx1ul432"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'santia_andia.txt'\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "banned_dict = {\"\\n\": \" \", \"á\": \"a\", \"é\": \"e\", \"í\": \"i\", \"ó\": \"o\", \"ú\": \"u\", \"#\":\" \", \"-\":\" \"}\n",
        "banned_dict\n",
        "# raw_text.replace()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX5rJsSnaZw6",
        "outputId": "2447a5c2-5c40-4cc4-c008-59c2ac81f4d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': ' ',\n",
              " 'á': 'a',\n",
              " 'é': 'e',\n",
              " 'í': 'i',\n",
              " 'ó': 'o',\n",
              " 'ú': 'u',\n",
              " '#': ' ',\n",
              " '-': ' '}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multipleReplace(text, wordDict):\n",
        "    for key in wordDict:\n",
        "        text = text.replace(key, wordDict[key])\n",
        "    return text\n",
        "\n",
        "multipleReplace(raw_text[1100:1220], banned_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gHJjF1xFgFeM",
        "outputId": "004133e7-a99e-42bc-a78e-ee68ec2174f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iii.  el capitan zaldumbide iv.  de otras personas distinguidas que formaban la tripulacion de «el dragon» v.  los dos t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bBfARqXn-tmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35319a3f-b52a-41eb-b7f9-e323b4579795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  490177\n",
            "Total Vocab:  59\n",
            "Total Patterns:  490077\n",
            "Epoch 1/10\n",
            "958/958 [==============================] - 91s 86ms/step - loss: 2.7562\n",
            "Epoch 2/10\n",
            "958/958 [==============================] - 86s 89ms/step - loss: 2.4850\n",
            "Epoch 3/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 2.3349\n",
            "Epoch 4/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 2.2218\n",
            "Epoch 5/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 2.1335\n",
            "Epoch 6/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 2.0664\n",
            "Epoch 7/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 2.0131\n",
            "Epoch 8/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 1.9691\n",
            "Epoch 9/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 1.9311\n",
            "Epoch 10/10\n",
            "958/958 [==============================] - 87s 91ms/step - loss: 1.8977\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7472ff1570>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load ascii text and covert to lowercase\n",
        "filename = '/content/santia_andia.txt'\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = multipleReplace(raw_text.lower(), banned_dict)\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "# filename = weights\n",
        "# model.load_weights(filename)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# model.load_weights('/content/weights-improvement-20-1.5095-bigger.hdf5')\n",
        "# define the checkpoint\n",
        "# filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "# callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "# model.fit(X, y, epochs=1, batch_size=512, callbacks=callbacks_list)\n",
        "model.fit(X, y, epochs=10, batch_size=512)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Has the model learnt anything?"
      ],
      "metadata": {
        "id": "ERY7_svHt1BD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "def generate_text(output_len: int, model: keras.engine.sequential.Sequential):\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  understandable_seed = ''.join([int_to_char[value] for value in pattern])\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", understandable_seed, \"\\\"\")\n",
        "  print(f\"You are generating an output text with len {output_len} chars\")\n",
        "  # print(\" END SEED\")\n",
        "  t_p = list()\n",
        "  print(\"Resulting Sentence\")\n",
        "  # generate characters\n",
        "  for i in range(output_len):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    # sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "    t_p.append(result)\n",
        "  # print(\"\\nDone.\")\n",
        "  understandable_result = \"\".join(t_p)\n",
        "  # print(result)\n",
        "  print(\"\\n\")\n",
        "  print(f\"Resulting sentence: {understandable_result}\")\n",
        "  # print(\"\\nDone.\")\n",
        "  # return understandable_result\n",
        "\n",
        "\n",
        "generate_text(1000, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSB6e35Co-4t",
        "outputId": "18820732-af2f-43ea-dae6-cce7997b3d90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\"  proximas al acantilado, se veian a la luz de la luna.  frayburu seguia en su desolacion y en su tri \"\n",
            "You are generating an output text with len 1000 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: ste de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la cara de la \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" The model has not learnt much :( \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7TxSQhdCYab",
        "outputId": "fcb85468-758a-4d3e-b427-a5bb9a56093b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The model has not learnt much :( \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "filename = '/content/santia_andia.txt'\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = multipleReplace(raw_text.lower(), banned_dict)\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_xCBPKJFB67",
        "outputId": "06bc4fd0-abdf-4df8-80ac-61af65423be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  490177\n",
            "Total Vocab:  59\n",
            "Total Patterns:  490077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "opt = Adam(learning_rate=0.0000125)\n",
        "\n",
        "filename = weights\n",
        "model.load_weights(filename)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)\n",
        "\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(100):\n",
        "  x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = np.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpimUZLdaImo",
        "outputId": "a5b1121d-f7af-446d-ee77-590e9cc26a5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7655/7658 [============================>.] - ETA: 0s - loss: 1.1342\n",
            "Epoch 1: loss improved from inf to 1.13413, saving model to weights-improvement-01-1.1341-bigger.hdf5\n",
            "7658/7658 [==============================] - 142s 18ms/step - loss: 1.1341\n",
            "Epoch 2/10\n",
            "7656/7658 [============================>.] - ETA: 0s - loss: 1.1347\n",
            "Epoch 2: loss did not improve from 1.13413\n",
            "7658/7658 [==============================] - 141s 18ms/step - loss: 1.1347\n",
            "Epoch 3/10\n",
            "7658/7658 [==============================] - ETA: 0s - loss: 1.1347\n",
            "Epoch 3: loss did not improve from 1.13413\n",
            "7658/7658 [==============================] - 142s 19ms/step - loss: 1.1347\n",
            "Epoch 4/10\n",
            "7658/7658 [==============================] - ETA: 0s - loss: 1.1354\n",
            "Epoch 4: loss did not improve from 1.13413\n",
            "7658/7658 [==============================] - 141s 18ms/step - loss: 1.1354\n",
            "Epoch 5/10\n",
            "7656/7658 [============================>.] - ETA: 0s - loss: 1.1346\n",
            "Epoch 5: loss did not improve from 1.13413\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1347\n",
            "Epoch 6/10\n",
            "7657/7658 [============================>.] - ETA: 0s - loss: 1.1332\n",
            "Epoch 6: loss improved from 1.13413 to 1.13317, saving model to weights-improvement-06-1.1332-bigger.hdf5\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1332\n",
            "Epoch 7/10\n",
            "7657/7658 [============================>.] - ETA: 0s - loss: 1.1332\n",
            "Epoch 7: loss did not improve from 1.13317\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1332\n",
            "Epoch 8/10\n",
            "7658/7658 [==============================] - ETA: 0s - loss: 1.1339\n",
            "Epoch 8: loss did not improve from 1.13317\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1339\n",
            "Epoch 9/10\n",
            "7657/7658 [============================>.] - ETA: 0s - loss: 1.1340\n",
            "Epoch 9: loss did not improve from 1.13317\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1341\n",
            "Epoch 10/10\n",
            "7655/7658 [============================>.] - ETA: 0s - loss: 1.1337\n",
            "Epoch 10: loss did not improve from 1.13317\n",
            "7658/7658 [==============================] - 140s 18ms/step - loss: 1.1337\n",
            "Seed:\n",
            "\" elfin destaca su cuerpo y sus aletas negras en el agua.  ese espectaculo de las olas, tan pronto tra \"\n",
            "s el mar de las casas de la calara del capitan y la carrasa en la cabeza.  el capitan no era un homb\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpuM5UPhKNYw",
        "outputId": "b8058977-1dda-4d9d-9758-1a451f54bb8b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" n _el dragon._  allen era un buen muchacho, pero muy poco marino. por mas que yo intente explicarle  \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: con el capitan y de la casa de la casa de la casa de la casa de la carretera.    ¿y que ha contestado?    no,    ¿pues este?    no.    ¿pue pasa?    si.    ¿pueses que haber?    no, no. no le parece marida.    ¿que puesesa   dijo dilos a iacer la cara de la cara de la cala     en la casa de la casa \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW1ISSA3APDb",
        "outputId": "2da66475-4983-4e8c-ed6a-75acdd5a0da6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" alizar en zalamerias.  cuando me dejaban entrar en la sala, me pasaba el tiempo mirandolos y diciend \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: o que el capitan era un hombre de aguirre, el capitan sandow era un capitan en la casa de la casa de la casa de la cala de la calle de la carreta.    ¿que te pasaba?  le dije yo  . es pue esta con el capitan sandow que se ha la cara de caples a la calle de la cara de la cara.    ¿tabes a que se le p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_overfit(input_seq_test: str, corpus: str = raw_text, chars_ahead: int = 150):\n",
        "  i_0 = corpus.find(input_seq_test)\n",
        "  i_f = i_0 + chars_ahead\n",
        "  print(corpus[i_0:i_f])\n",
        "\n",
        "q = \"yo no se si estara bien..., porque las g\"\n",
        "is_overfit(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM3Lu5NtAXH0",
        "outputId": "9f177845-22ae-4ef4-cd9b-3ad0fd549047"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yo no se si estara bien..., porque las gentes diran que ...    eso ya os lo he dicho antes. la muchacha esta en ese estado. ya lo sabemos. conque reso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"quiza le remordian sus crimenes.  antes de ser n\"\n",
        "is_overfit(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WndoIFF6ud5A",
        "outputId": "0c378fe0-b338-4cbb-e3b4-d091470d75e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quiza le remordian sus crimenes.  antes de ser negrero, el viejo, segun decian, habia hecho naufragar varios barcos asegurados, llegando hasta exponer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "id": "s7-YSofbKzoM",
        "outputId": "7a1d9943-c94b-4eb4-81e0-a2aa99a1478f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" iria nada, sino que si veia algun espia en la finca lo zambulliria en el estanque.  salimos de alla; \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence:  en el camino de la casa era un caritan de la calle de la calle de la carretera y de la carretera y la de las animas.  el capitan se habia conteduido de nada, me dijo que era una manera deserperada y me dijo:    no habra si la seña estan el mar.  el capitan se habia conteduo de su padre, el capitan \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "id": "6v4bzWds08XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d34a29-6ccf-4f2e-9ac5-88c1ada94da0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" en el cementerio de la aldea, y tendidos entre dos sepulcros, resguardados del viento, pudimos desca \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: rsar en la cabeza.  el capitan no era un hombre de aguirre, el capitan sandow era un capitan de la casa de la casa de la casa de la carretera.  el capitan no era un hombre de aguirre, el capitan sandow era un capitan de la casa de la casa de la casa de la carretera.  el capitan no era un hombre de a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"resguardados del viento, pudimos desca\"\n",
        "is_overfit(q)"
      ],
      "metadata": {
        "id": "0TO8U5zGLyTf",
        "outputId": "9aa7513d-2bcf-469a-a5c2-e3b090de5f39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resguardados del viento, pudimos descansar y dormir.  a media noche nos despertamos de hambre y de frio. nos levantamos, salimos del cementerio y echa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What happens when we retrain on new data?"
      ],
      "metadata": {
        "id": "RiPPE90PO9Dv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "weights = subprocess.getoutput('ls | grep \"weights\"')\n",
        "print(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DEuelMUFUr",
        "outputId": "23b770d6-8bf1-4b15-e10b-e8a12cff2be8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights-improvement-01-1.1439-bigger.hdf5\n",
            "weights-improvement-03-1.1425-bigger.hdf5\n",
            "weights-improvement-04-1.1423-bigger.hdf5\n",
            "weights-improvement-15-1.1389-bigger.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg16A7BqRRfp",
        "outputId": "d9af0b9b-326e-43c5-bd80-ca7cb7eceb27"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" ierta, proxima a un pueblecito que tenia su puerto.  yo habia oido decir que en algunos puntos de es \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: cribir el camino de la casa de la casa en el camino de la calara del capitan y la carrasa en la cabeza.  el capitan no era un hombre de aguirre, el capitan sandow era un capitan con un cartillo de proa, y estaba alguna de las casas de la carreta.  el capitan no era un hombre de aguirre, el capitan s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"yo habia oido decir que en algunos puntos de es\"\n",
        "is_overfit(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcjPw7X6e_Tb",
        "outputId": "28c9ca6b-4dee-44a4-c1e6-81d7e3c2c8ea"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yo habia oido decir que en algunos puntos de escocia y de irlanda comen esas algas que se llaman laminarias, y era tal nuestra hambre, que intentamos \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_new_file(text: str, previous_chars: dict) -> str:\n",
        "  text_ = \"\".join([char for char in text if char in previous_chars.keys()])\n",
        "  return text_"
      ],
      "metadata": {
        "id": "xRf8UrwShXKW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" we need the new corpus to have the same characters as the previous one\")"
      ],
      "metadata": {
        "id": "b2tUSNgsXenE",
        "outputId": "11a45c0c-1d47-4774-a565-942c3ce07937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " we need the new corpus to have the same characters as the previous one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = '/content/filosofia_fundamental'\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = multipleReplace(raw_text.lower(), banned_dict)\n",
        "raw_text = clean_new_file(raw_text, char_to_int)\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-06-1.1332-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "\n",
        "opt = Adam(learning_rate=0.001225)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X, y, epochs=10, batch_size=512, callbacks=callbacks_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evl8SGPEgi6v",
        "outputId": "c30a045a-5d4a-45dd-9565-3abaaf4c4481"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  501821\n",
            "Total Vocab:  59\n",
            "Total Patterns:  501721\n",
            "Epoch 1/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.7831\n",
            "Epoch 1: loss improved from inf to 1.78312, saving model to weights-improvement-01-1.7831-bigger.hdf5\n",
            "980/980 [==============================] - 91s 90ms/step - loss: 1.7831\n",
            "Epoch 2/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.5405\n",
            "Epoch 2: loss improved from 1.78312 to 1.54050, saving model to weights-improvement-02-1.5405-bigger.hdf5\n",
            "980/980 [==============================] - 89s 90ms/step - loss: 1.5405\n",
            "Epoch 3/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.4590\n",
            "Epoch 3: loss improved from 1.54050 to 1.45901, saving model to weights-improvement-03-1.4590-bigger.hdf5\n",
            "980/980 [==============================] - 89s 90ms/step - loss: 1.4590\n",
            "Epoch 4/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.4084\n",
            "Epoch 4: loss improved from 1.45901 to 1.40836, saving model to weights-improvement-04-1.4084-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.4084\n",
            "Epoch 5/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.3709\n",
            "Epoch 5: loss improved from 1.40836 to 1.37089, saving model to weights-improvement-05-1.3709-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.3709\n",
            "Epoch 6/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.3426\n",
            "Epoch 6: loss improved from 1.37089 to 1.34261, saving model to weights-improvement-06-1.3426-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.3426\n",
            "Epoch 7/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.3194\n",
            "Epoch 7: loss improved from 1.34261 to 1.31937, saving model to weights-improvement-07-1.3194-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.3194\n",
            "Epoch 8/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.2975\n",
            "Epoch 8: loss improved from 1.31937 to 1.29754, saving model to weights-improvement-08-1.2975-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.2975\n",
            "Epoch 9/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.2832\n",
            "Epoch 9: loss improved from 1.29754 to 1.28320, saving model to weights-improvement-09-1.2832-bigger.hdf5\n",
            "980/980 [==============================] - 89s 91ms/step - loss: 1.2832\n",
            "Epoch 10/10\n",
            "980/980 [==============================] - ETA: 0s - loss: 1.2658\n",
            "Epoch 10: loss improved from 1.28320 to 1.26579, saving model to weights-improvement-10-1.2658-bigger.hdf5\n",
            "980/980 [==============================] - 88s 90ms/step - loss: 1.2658\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f73d824d060>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dg57pkNgjXO",
        "outputId": "7aacfb2d-9dba-41d0-b720-1865ac91262b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" udes determinadas, y estas en cierta relacion fija, que nosotros calculamos, comparandola con una ex \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: tension en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espaci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(300, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57BxSDd3gjc4",
        "outputId": "458c2028-5d74-429e-bf22-1d719d314ac5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" pesado eso que llamamos realidad. estoy en un mundo donde yo mando, quiero: y el coche esta pronto,  \"\n",
            "You are generating an output text with len 300 chars\n",
            "Resulting Sentence\n",
            "\n",
            "\n",
            "Resulting sentence: es el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el espacio en el \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Despite the loss metric has improved Model has lost its touch\")"
      ],
      "metadata": {
        "id": "rF-p9uBuLYT6",
        "outputId": "dfed6159-3dae-4b60-c2e5-53c87a03497d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Despite the loss metric has improved Model has lost its touch\n"
          ]
        }
      ]
    }
  ]
}